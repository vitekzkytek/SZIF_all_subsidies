{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd \n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parseUrl(url):\n",
    "    '''\n",
    "    Given an url adress (e.g.: https://www.szif.cz/cs/seznam-prijemcu-dotaci?ji=1000644235&opatr=&year=2017&portalAction=detail)\n",
    "    it downloads all the tables (i.e. div element classed as 'container-table') in the documentand returns as a single dataframe.\n",
    "    '''\n",
    "    # Create empty list\n",
    "    dfs = []\n",
    "    \n",
    "    #Download the webpage URL\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text)\n",
    "    \n",
    "    # Get all DIV elements classed as 'container-table'\n",
    "    tbls = soup.findAll('div', {'class':'container-table'})\n",
    "    \n",
    "    # For each table in document create a pandas dataframe and append to list\n",
    "    for tbl in tbls:\n",
    "        dfs.append(parseTbl(tbl))\n",
    "    \n",
    "    if len(dfs) > 0:\n",
    "        # Merge all tables into a single dataframe\n",
    "        df = pd.concat(dfs,sort=True)\n",
    "        \n",
    "        #Příjemce is in the h3 element inside the div element classed as section\n",
    "        df.loc[:,'Příjemce'] = soup.find('div',{'class':'section'}).find('h3').text\n",
    "        \n",
    "        # Sídlo is also in the div (class: section), and is in the independent div element with no class\n",
    "        df.loc[:,'Sídlo'] = soup.find('div',{'class':'section'}).find('div',{'class':None}).text\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        # If no tables found, print error and return nothing.\n",
    "        print('Error: No table found in Příjemce {}'.format(soup.find('div',{'class':'section'}).find('h3').text))\n",
    "    \n",
    "def parseTbl(soup):\n",
    "    '''\n",
    "    Given BeautifulSoup object of a table it converts it into the pandas dataframe. First row is considered a header\n",
    "    Returns: Pandas dataframe\n",
    "    '''\n",
    "    trs = soup.findAll('tr')\n",
    "    l = []\n",
    "    for tr in trs[1:]:\n",
    "        td = tr.find_all('td')\n",
    "        row = [tr.text for tr in td]\n",
    "        l.append(row)\n",
    "    df = pd.DataFrame(l,columns=[th.text for th in trs[0].findAll('th')])\n",
    "    return df\n",
    "\n",
    "\n",
    "def parseSinglePageList(url):\n",
    "    '''\n",
    "    Finds all links in the page of links (e.g. https://www.szif.cz/cs/seznam-prijemcu-dotaci?asc=asc&year=2017&sortby=%2FBIC%2FZC_F201&ino=0&page=1 )\n",
    "    and downloads all tables in the all links on the page\n",
    "    '''\n",
    "    \n",
    "    # Downloads the page of links\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text)\n",
    "    \n",
    "    # Find a table with links\n",
    "    tbl = soup.find('div',{'class':'container-table'})\n",
    "    \n",
    "    #find all links\n",
    "    hrefs = [a['href'] for a in tbl.findAll('a')]\n",
    "    \n",
    "    # Add domain address and filter that does not start with '?ji='\n",
    "    links = ['https://www.szif.cz/cs/seznam-prijemcu-dotaci' + link for link in hrefs if link[:4] == '?ji=']\n",
    "    dfs = []\n",
    "    \n",
    "    for link in links:\n",
    "        #For each link download all tables\n",
    "        dfs.append(parseUrl(link))\n",
    "        \n",
    "        # sleep for 0.5s so that the server is not overloaded\n",
    "        sleep(0.5)\n",
    "        \n",
    "    # return all dataframes\n",
    "    return pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "Error: No table found in P\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "Error: No table found in P\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "for i in range(650):\n",
    "    # create a link for all 650 result pages\n",
    "    url = 'https://www.szif.cz/cs/seznam-prijemcu-dotaci?asc=asc&year=2017&sortby=%2FBIC%2FZC_F201&ino=0&page={}'.format(i)\n",
    "    \n",
    "    # for each result page, download all tables in all links and add to the list of dataframes\n",
    "    dfs.append(parseSinglePageList(url))\n",
    "    \n",
    "    #print\n",
    "    print(i)\n",
    "    \n",
    "    #wait for 5 seconds, so that server is not overloaded\n",
    "    sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(dfs)\n",
    "df.to_csv('SZIF_all_receivers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
